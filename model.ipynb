{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from functions import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pytorch_util import RAdam,trainable_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 256\n",
    "batch_size = 8\n",
    "epochs = 10\n",
    "clip = 1.0\n",
    "lr = 4e-4\n",
    "encoder_str = 'efficientnet-b2'\n",
    "decoder_channels=(256, 128, 64, 32, 16)\n",
    "decoder_repeats=(2,2,2,2,2)\n",
    "includeX0 = False\n",
    "opt_level=\"O1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"../Data/pickles_\"+str(sz)+\"/images/\"\n",
    "masks_dir = r\"../Data/pickles_\"+str(sz)+\"/masks/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../Data/imageId.pickle\", \"rb\") as output_file:\n",
    "    imageId = pickle.load(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup train/val\n",
    "imageId_val = imageId[:2400]\n",
    "imageId_train = imageId[2400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sz in preprocessing_dict:\n",
    "    dataset_train = dataset(imageId_train,images_dir,masks_dir,transform,preprocessing_dict[sz])\n",
    "else:\n",
    "    dataset_train = dataset(imageId_train,images_dir,masks_dir,transform)\n",
    "    dataset_train._cal_preprocessing()\n",
    "dataset_val = dataset(imageId_val,images_dir,masks_dir,preprocessing=dataset_train.preprocessing)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs,masks = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b2\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "# freeze encoder\n",
    "encoder = EfficientNet_encoder.from_pretrained(encoder_str,includeX0=includeX0)\n",
    "set_requires_grad(encoder,False)\n",
    "decoder = EfficientNet_decoder(sz,encoder_channels[encoder_str],decoder_channels=decoder_channels,decoder_repeats=decoder_repeats)\n",
    "model = Unet(encoder,decoder).to('cuda')\n",
    "paras = trainable_parameter(model)\n",
    "opt = RAdam(paras,lr=lr,weight_decay=1e-2)\n",
    "scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-05)\n",
    "model, opt = amp.initialize(model, opt, opt_level=opt_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss: +1.314, val_loss: +1.036\n",
      "\n",
      "epoch:1, train_loss: +0.810, val_loss: +0.757\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "epoch:2, train_loss: +0.741, val_loss: +0.688\n",
      "\n",
      "epoch:3, train_loss: +0.722, val_loss: +0.640\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "epoch:4, train_loss: +0.701, val_loss: +0.631\n",
      "\n",
      "epoch:5, train_loss: +0.684, val_loss: +0.618\n",
      "\n",
      "epoch:6, train_loss: +0.677, val_loss: +0.605\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "epoch:7, train_loss: +0.664, val_loss: +0.628\n",
      "\n",
      "epoch:8, train_loss: +0.663, val_loss: +0.619\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "epoch:9, train_loss: +0.649, val_loss: +0.594\n",
      "\n",
      "Training completed in 3953.82515001297s\n"
     ]
    }
   ],
   "source": [
    "model,bestWeight,bestOpt,bestAmp = train(opt,model,epochs,train_loader,\\\n",
    "                                 valid_loader,paras,clip,scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model': bestWeight,\n",
    "    'opt': bestOpt,\n",
    "    'amp': bestAmp\n",
    "}\n",
    "torch.save(checkpoint, '../Model/'+str(sz)+'_'+encoder_str+'_'+str(decoder_channels)+'_'+str(decoder_repeats)+'_pretrain.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b2\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "encoder = EfficientNet_encoder.from_pretrained(encoder_str,includeX0=includeX0)\n",
    "set_requires_grad(encoder,False)\n",
    "decoder = EfficientNet_decoder(sz,encoder_channels[encoder_str],decoder_channels=decoder_channels,decoder_repeats=decoder_repeats)\n",
    "model = Unet(encoder,decoder).to('cuda')\n",
    "paras = trainable_parameter(model)\n",
    "opt = RAdam(paras,lr=lr,weight_decay=1e-2)\n",
    "scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-05)\n",
    "model, opt = amp.initialize(model, opt, opt_level=opt_level)\n",
    "\n",
    "checkpoint = torch.load('../Model/'+str(sz)+'_'+encoder_str+'_'+str(decoder_channels)+'_'+str(decoder_repeats)+'_pretrain.pt')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "opt.load_state_dict(checkpoint['opt'])\n",
    "amp.load_state_dict(checkpoint['amp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "epoch:0, train_loss: +0.619, val_loss: +0.574\n",
      "\n",
      "epoch:1, train_loss: +0.618, val_loss: +0.566\n",
      "\n",
      "epoch:2, train_loss: +0.612, val_loss: +0.575\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "epoch:3, train_loss: +0.599, val_loss: +0.555\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "epoch:4, train_loss: +0.587, val_loss: +0.547\n",
      "\n",
      "epoch:5, train_loss: +0.577, val_loss: +0.532\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "epoch:6, train_loss: +0.572, val_loss: +0.530\n",
      "\n",
      "epoch:7, train_loss: +0.557, val_loss: +0.531\n",
      "\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "epoch:8, train_loss: +0.567, val_loss: +0.519\n",
      "\n",
      "epoch:9, train_loss: +0.550, val_loss: +0.520\n",
      "\n",
      "Training completed in 4945.2053327560425s\n"
     ]
    }
   ],
   "source": [
    "# unfreeze encoder with opt reset\n",
    "set_requires_grad(encoder,True)\n",
    "paras = trainable_parameter(model)\n",
    "opt = RAdam(paras,lr=lr*0.25,weight_decay=1e-2)\n",
    "scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-05*0.1)\n",
    "model, opt = amp.initialize(model, opt, opt_level=opt_level)\n",
    "\n",
    "model,bestWeight,bestOpt,bestAmp = train(opt,model,epochs,train_loader,valid_loader,paras,clip,scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model': bestWeight,\n",
    "    'opt': bestOpt,\n",
    "    'amp': bestAmp\n",
    "}\n",
    "torch.save(checkpoint, '../Model/'+str(sz)+'_'+encoder_str+'_'+str(decoder_channels)+'_'+str(decoder_repeats)+'_FT1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze encoder\n",
    "set_requires_grad(encoder,True)\n",
    "opt.add_param_group({'params': encoder.parameters()})\n",
    "for param_group in opt.param_groups:\n",
    "    param_group['lr'] = lr * 0.25\n",
    "model,bestWeight,bestOpt,bestAmp = train(opt,model,epochs,train_loader,valid_loader,paras,clip,scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b2\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "encoder = EfficientNet_encoder.from_pretrained(encoder_str,includeX0=includeX0)\n",
    "decoder = EfficientNet_decoder(sz,encoder_channels[encoder_str],decoder_channels=decoder_channels,decoder_repeats=decoder_repeats)\n",
    "model = Unet(encoder,decoder).to('cuda')\n",
    "paras = trainable_parameter(model)\n",
    "opt = RAdam(paras,lr=lr,weight_decay=1e-2)\n",
    "scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=5,min_lr=1e-05)\n",
    "model, opt = amp.initialize(model, opt, opt_level=opt_level)\n",
    "\n",
    "checkpoint = torch.load('../Model/'+str(sz)+'_'+encoder_str+'_'+str(decoder_channels)+'_'+str(decoder_repeats)+'_FT1.pt')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "opt.load_state_dict(checkpoint['opt'])\n",
    "amp.load_state_dict(checkpoint['amp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit TTA\n",
    "df = pd.read_csv('../Data/stage_2_sample_submission.csv')\n",
    "testID = df['ImageId'].values.tolist()\n",
    "yhat = predict_TTA(model,testID,images_dir,dataset_train.preprocessing,batch_size)\n",
    "\n",
    "best_threshold = 0.3\n",
    "min_size = 1000\n",
    "submit(df,yhat,best_threshold, min_size,\\\n",
    "       '../Submission/'+str(sz)+'_'+encoder_str+'_'+str(decoder_channels)+'_'+str(decoder_repeats)+'_FT1_TTA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for threshold and min_size on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = dataset(imageId_val,images_dir,preprocessing=dataset_train.preprocessing)\n",
    "valid_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = predict(model,valid_loader)\n",
    "if sz != 1024:\n",
    "    y_val = [cv2.resize(y_, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR) for y_ in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_list = [0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "min_size_list = [1000,3000,5000,7000,9000,11000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grids = GridSearch(threshold_list,min_size_list,y_val,imageId_val)\n",
    "Grids = pd.DataFrame(Grids,index=threshold_list,columns=min_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1000</th>\n",
       "      <th>3000</th>\n",
       "      <th>5000</th>\n",
       "      <th>7000</th>\n",
       "      <th>9000</th>\n",
       "      <th>11000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.3</td>\n",
       "      <td>0.794744</td>\n",
       "      <td>0.808674</td>\n",
       "      <td>0.811288</td>\n",
       "      <td>0.809157</td>\n",
       "      <td>0.801174</td>\n",
       "      <td>0.794023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.4</td>\n",
       "      <td>0.794468</td>\n",
       "      <td>0.809943</td>\n",
       "      <td>0.812008</td>\n",
       "      <td>0.808625</td>\n",
       "      <td>0.799971</td>\n",
       "      <td>0.792091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.5</td>\n",
       "      <td>0.796111</td>\n",
       "      <td>0.810194</td>\n",
       "      <td>0.810960</td>\n",
       "      <td>0.808410</td>\n",
       "      <td>0.799807</td>\n",
       "      <td>0.792338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.6</td>\n",
       "      <td>0.796337</td>\n",
       "      <td>0.810586</td>\n",
       "      <td>0.813576</td>\n",
       "      <td>0.805689</td>\n",
       "      <td>0.796857</td>\n",
       "      <td>0.791682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.7</td>\n",
       "      <td>0.797355</td>\n",
       "      <td>0.809860</td>\n",
       "      <td>0.812057</td>\n",
       "      <td>0.804541</td>\n",
       "      <td>0.797763</td>\n",
       "      <td>0.790615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.8</td>\n",
       "      <td>0.798425</td>\n",
       "      <td>0.811293</td>\n",
       "      <td>0.812484</td>\n",
       "      <td>0.803311</td>\n",
       "      <td>0.795428</td>\n",
       "      <td>0.788848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.9</td>\n",
       "      <td>0.799353</td>\n",
       "      <td>0.812115</td>\n",
       "      <td>0.810582</td>\n",
       "      <td>0.801403</td>\n",
       "      <td>0.792865</td>\n",
       "      <td>0.788191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1000      3000      5000      7000      9000      11000\n",
       "0.3  0.794744  0.808674  0.811288  0.809157  0.801174  0.794023\n",
       "0.4  0.794468  0.809943  0.812008  0.808625  0.799971  0.792091\n",
       "0.5  0.796111  0.810194  0.810960  0.808410  0.799807  0.792338\n",
       "0.6  0.796337  0.810586  0.813576  0.805689  0.796857  0.791682\n",
       "0.7  0.797355  0.809860  0.812057  0.804541  0.797763  0.790615\n",
       "0.8  0.798425  0.811293  0.812484  0.803311  0.795428  0.788848\n",
       "0.9  0.799353  0.812115  0.810582  0.801403  0.792865  0.788191"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f83ac1f8650>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(Grids.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for threshold and min_size on validation data with TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = predict_TTA(model,imageId_val,images_dir,dataset_train.preprocessing,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sz != 1024:\n",
    "    y_val = [cv2.resize(y_, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR) for y_ in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_list = [0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "min_size_list = [1000,3000,5000,7000,9000,11000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grids = GridSearch(threshold_list,min_size_list,y_val,imageId_val)\n",
    "Grids = pd.DataFrame(Grids,index=threshold_list,columns=min_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1000</th>\n",
       "      <th>3000</th>\n",
       "      <th>5000</th>\n",
       "      <th>7000</th>\n",
       "      <th>9000</th>\n",
       "      <th>11000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.3</td>\n",
       "      <td>0.787430</td>\n",
       "      <td>0.807274</td>\n",
       "      <td>0.816334</td>\n",
       "      <td>0.812435</td>\n",
       "      <td>0.801010</td>\n",
       "      <td>0.795511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.4</td>\n",
       "      <td>0.791103</td>\n",
       "      <td>0.809267</td>\n",
       "      <td>0.815725</td>\n",
       "      <td>0.810438</td>\n",
       "      <td>0.800877</td>\n",
       "      <td>0.795013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.5</td>\n",
       "      <td>0.807491</td>\n",
       "      <td>0.816576</td>\n",
       "      <td>0.814831</td>\n",
       "      <td>0.806703</td>\n",
       "      <td>0.799494</td>\n",
       "      <td>0.790619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.6</td>\n",
       "      <td>0.809147</td>\n",
       "      <td>0.815301</td>\n",
       "      <td>0.811042</td>\n",
       "      <td>0.802651</td>\n",
       "      <td>0.794687</td>\n",
       "      <td>0.788342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.7</td>\n",
       "      <td>0.810009</td>\n",
       "      <td>0.815106</td>\n",
       "      <td>0.809818</td>\n",
       "      <td>0.801212</td>\n",
       "      <td>0.793077</td>\n",
       "      <td>0.786766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.8</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>0.814548</td>\n",
       "      <td>0.808640</td>\n",
       "      <td>0.798579</td>\n",
       "      <td>0.790681</td>\n",
       "      <td>0.785479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.9</td>\n",
       "      <td>0.810062</td>\n",
       "      <td>0.813703</td>\n",
       "      <td>0.806513</td>\n",
       "      <td>0.796698</td>\n",
       "      <td>0.788728</td>\n",
       "      <td>0.783396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1000      3000      5000      7000      9000      11000\n",
       "0.3  0.787430  0.807274  0.816334  0.812435  0.801010  0.795511\n",
       "0.4  0.791103  0.809267  0.815725  0.810438  0.800877  0.795013\n",
       "0.5  0.807491  0.816576  0.814831  0.806703  0.799494  0.790619\n",
       "0.6  0.809147  0.815301  0.811042  0.802651  0.794687  0.788342\n",
       "0.7  0.810009  0.815106  0.809818  0.801212  0.793077  0.786766\n",
       "0.8  0.809300  0.814548  0.808640  0.798579  0.790681  0.785479\n",
       "0.9  0.810062  0.813703  0.806513  0.796698  0.788728  0.783396"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f83a1d95910>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(Grids.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = 0.5\n",
    "min_size = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/stage_2_sample_submission.csv')\n",
    "testID = df['ImageId'].values.tolist()\n",
    "dataset_test = dataset(testID,images_dir,preprocessing=dataset_train.preprocessing)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "yhat = predict(model,test_loader)\n",
    "\n",
    "submit(df,yhat,best_threshold, min_size,\\\n",
    "       '../Submission/'+str(sz)+'_'+encoder_str+'_'+str(decoder_channels)+'_'+str(decoder_repeats)+'_FT1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit TTA\n",
    "df = pd.read_csv('../Data/stage_2_sample_submission.csv')\n",
    "testID = df['ImageId'].values.tolist()\n",
    "yhat = predict_TTA(model,testID,images_dir,dataset_train.preprocessing,batch_size)\n",
    "\n",
    "submit(df,yhat,best_threshold, min_size,\\\n",
    "       '../Submission/'+str(sz)+'_'+encoder_str+'_'+str(decoder_channels)+'_'+str(decoder_repeats)+'_FT1_TTA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
